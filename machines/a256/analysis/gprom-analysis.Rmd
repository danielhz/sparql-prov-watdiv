---
title: "GProM Watdiv benchmarks"
output: html_notebook
---

# Loading libraries

```{r}
library(plotly)
library(dplyr)
library(ggplot2)
library(latex2exp)
library(plotrix)
library(scales)
library(plyr)
library(purrr)
library(data.table)
library(patternplot)
library(gcookbook)
```

# Defining functions

```{r}
get.se <- function(y) {
 se <- sd(y)/sqrt(length(y))
 mu <- mean(y)
 c(ymin=mu-se, ymax=mu+se)
}


# Function to calculate the mean and the standard deviation for each group.
# data : a data frame
# varname : the name of a column containing the variable to be summariezed
# groupnames : vector of column names to be used as grouping variables
data_summary <- function(data, varname, groupnames) {
  require(plyr)
  summary_func <- function(x, col) {
    c(mean=mean(x[[col]], na.rm=TRUE), sd=sd(x[[col]], na.rm=TRUE))
  }
  data_sum<-ddply(data, groupnames, .fun=summary_func, varname)
  data_sum <- rename(data_sum, c("mean" = varname))
  return(data_sum)
}

## Summarizes data.
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    library(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}
```

# Read, clean, and prepare the results

First, we read the result data files.  For this analysis we consider only the
dataset of size 100M triples.

```{r}
gprom_results <- 
  list.files(path = "../../../results",
             pattern = "sqlite.*.csv",
             full.names = TRUE) %>% 
  map_df(~fread(.))
```

# Plot files with averages

```{r}
p1 <- ggplot(gprom_results %>%
               mutate(time = time * 1000) %>%
               mutate(scheme_mode = paste(scheme, mode)),
       aes(x=template, y=time, fill=scheme_mode)) +
  stat_summary(fun=mean, geom="bar", size = 0.1,
               position=position_dodge(preserve = "single"),
               color="black") +
  stat_summary(fun.data=get.se, geom="errorbar", width=0.5,
               position=position_dodge(width = 0.9, preserve = "single")) +
  scale_y_continuous(trans='log10', labels = trans_format("log10", math_format(10^.x))) +
  xlab("query template") + ylab("time (ms)") +
  theme_bw(base_family = "Times") +
  labs(fill = "scheme and mode:")

p1
```




