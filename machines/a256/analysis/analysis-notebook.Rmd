---
title: "Watdiv experiment"
output: html_notebook
---

# Load the data

In this section we load the data and generate the dataframes we analyze throughout this notebook.

We next load the required libraries for the remained of this notebook.

```{r}
library(plotly)
library(dplyr)
library(ggplot2)
library(latex2exp)
library(plotrix)
library(scales)
library(plyr)
library(purrr)
library(data.table)
library(patternplot)
library(gcookbook)
```

# Define some functions

```{r}
get.se <- function(y) {
 se <- sd(y)/sqrt(length(y))
 mu <- mean(y)
 c(ymin=mu-se, ymax=mu+se)
}


# Function to calculate the mean and the standard deviation for each group.
# data : a data frame
# varname : the name of a column containing the variable to be summariezed
# groupnames : vector of column names to be used as grouping variables
data_summary <- function(data, varname, groupnames) {
  require(plyr)
  summary_func <- function(x, col) {
    c(mean=mean(x[[col]], na.rm=TRUE), sd=sd(x[[col]], na.rm=TRUE))
  }
  data_sum<-ddply(data, groupnames, .fun=summary_func, varname)
  data_sum <- rename(data_sum, c("mean" = varname))
  return(data_sum)
}

## Summarizes data.
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    library(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}
```

# Read, clean, and prepare the results

First, we read the result data files.  For this analysis we consider only the
dataset of size 100M triples.

```{r}
time_results <- 
  list.files(path = "../../../results",
             pattern = "*.csv",
             full.names = TRUE) %>% 
  map_df(~fread(.)) %>%
  filter(size == "100M")
```

There are some cases of timeout where the time can be different to 300. This
cases are:

1. The server produces a timeout with a non-200 status, but the client records
   a time that is different to 300s. It could be even shorter than 300s.

2. The server client did not produces a timeout, so the client stoped the the
   process and gives a time longer than 300s, but with status 200.

```{r}
time_results$time[time_results$time >= 260 | time_results$status != 200] <- 300
```

In the data we use lowercase keywords and omit some spaces between words.  In
the plots we capitalize the names of the engines and introduce spaces properly.

```{r}
time_results$engine[time_results$engine == "fuseki"] <- "Fuseki"
time_results$engine[time_results$engine == "virtuoso"] <- "Virtuoso"
time_results$engine[time_results$engine == "tripleprov"] <- "TripleProv"
time_results$scheme[time_results$scheme == "namedgraphs"] <- "named graphs"
time_results$scheme[time_results$scheme == "rdf"] <- "standard"
time_results$scheme[time_results$scheme == "wikidata"] <- "Wikidata"
# time_results$scheme[time_results$scheme == "PSQ"] <- "named graphs"
```

We want to group results considering the engine and the mode.

```{r}
time_results$group <- paste(time_results$engine, time_results$mode)

# For TripleProv we do not need to mention the reification engine.
time_results$group[time_results$engine == "TripleProv"] <- "TripleProv"
```

# We next describ


# We next plot the solutions based on SQL using GProm

```{r}
sql_results <- time_results[time_results$engine == 'sqlite']
```


# We next plot the solutions based on SPARQL

```{r}
labels <- c("Fuseki B", "Fuseki R", "Fuseki P",
            "Virtuoso B", "Virtuoso R", "Virtuoso P",
            "TripleProv", "sqlite SQL")

time_results$fill_position[time_results$group == "Fuseki B"] <- 1
time_results$fill_position[time_results$group == "Fuseki R"] <- 2
time_results$fill_position[time_results$group == "Fuseki P"] <- 3
time_results$fill_position[time_results$group == "Virtuoso B"] <- 4
time_results$fill_position[time_results$group == "Virtuoso R"] <- 5
time_results$fill_position[time_results$group == "Virtuoso P"] <- 6
time_results$fill_position[time_results$group == "TripleProv"] <- 7
time_results$fill_position[time_results$group == "sqlite SQL"] <- 7

positions <- c(1, 2, 3, 4, 5, 6, 7)

p1_data <- time_results
p1_data$time <- with(p1_data, time * 1000)

p1 <- ggplot(p1_data,
       aes(x=template, y=time, fill=reorder(group, fill_position))) +
  facet_wrap(~scheme, ncol = 3) +
  stat_summary(fun=mean, geom="bar", size = 0.1,
               position=position_dodge(preserve = "single"),
               color="black") +
  # stat_summary(fun.data=get.se, geom="errorbar", width=0.5,
  #              position=position_dodge(width = 0.9, preserve = "single")) +
  geom_hline(yintercept=300000, color="black", linetype=3, size=0.4) +
  scale_y_continuous(trans='log10', labels = trans_format("log10", math_format(10^.x))) +
  xlab("query template") + ylab("runtime (ms)") +
  scale_fill_manual(values = c("Fuseki B" = "#ece7f2",
                               "Fuseki R" = "#a6bddb",
                               "Fuseki P" = "#2b8cbe",
                               "Virtuoso B" = "#fee8c8",
                               "Virtuoso R" = "#fdbb84",
                               "Virtuoso P" = "#e34a33",
                               "TripleProv" = "#8856a7",
                               "sqlite SQL" = "#881111"),
                    breaks = c("Fuseki B",
                               "Fuseki R",
                               "Fuseki P",
                               "Virtuoso B",
                               "Virtuoso R",
                               "Virtuoso P",
                               "TripleProv",
                               "sqlite SQL")) +
  theme_minimal(base_family = "Times") +
  labs(fill = "") +
  theme(legend.position="top",
        legend.box.margin = margin(0, 0, 0, 0, "pt"),
        legend.box.spacing = unit(0, "pt"),
        legend.box = "horizontal") +
  guides(fill = guide_legend(nrow = 1))
p1

png(file="watdiv-times.png", res=600, width=6000, height=1500, pointsize=8)
  p1
dev.off()

ggplotly(p1)

#p2 <- patternbar(time_results, time_results$template, time_results$time, group=NULL)
```
# Computing the max overhead

We summarize time results by query_id for all queries.

```{r}
summarized_results <- data_summary(time_results,
                                   varname="time",
                                   groupnames=c("template", "engine", "scheme",
                                                "query_id", "mode"))
```

We separate times and perform the join.

```{r}
data_B <- summarized_results %>% filter(mode == "B") %>% select(-"mode", -"sd")
data_R <- summarized_results %>% filter(mode == "R") %>% select(-"mode", -"sd")
data_P <- summarized_results %>% filter(mode == "P") %>% select(-"mode", -"sd")

names(data_B)[names(data_B)=="time"] <- "time_B"
names(data_R)[names(data_R)=="time"] <- "time_R"
names(data_P)[names(data_P)=="time"] <- "time_P"

overhead_data <- data_B %>% join(data_R) %>% join(data_P)
overhead_data$ratio_RB <- overhead_data$time_R/overhead_data$time_B
overhead_data$ratio_PB <- overhead_data$time_P/overhead_data$time_B
overhead_data$ratio_PR <- overhead_data$time_P/overhead_data$time_R

# This is the maximum ratio.
max_ratio_PR <- overhead_data %>%
  filter(template != "C3" & template != "S3" & scheme != "standard") # %>%
  #select("ratio_PR") #%>%
  #max()

paste("Maximum ratio:", max_ratio_PR %>% select("ratio_PR") %>% max())
```

